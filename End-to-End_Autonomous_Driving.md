# 端到端自动驾驶
上一代算法架构：**Data** -> **Perception** -> **Planner**  

上一代算法架构的缺陷：
* 感知不准
* 规控不拟人，覆盖不了长尾问题
* 感知-规控的衔接有信息损耗

端到端算法架构：**Data** -> **End2End(Planner)** （既干感知算法又干规控算法）   

多模态大语言模型的兴起，让人们看到信息大融合带来的无限创造力

四大分支方向：
1. MLLM（多模态大语言模型）
2. 世界模型
3. VLA（Vision Language Action Model）
4. 扩散模型

## 端到端算法分类
### 两段式端到端
算法架构：**Data** -> **Perception** -> **Planner**  
* 传统算法框架到端到端算法的过渡阶段
* Planner模型化，解决线性planner重度依赖规则的问题（让Planner变得更强，能逐渐把Perception融入Planner）
* 仍具有现行架构的感知不准、信息损耗等问题
* 适合新手入门
### 一段式端到端
算法架构：**Data** -> **End2End(Planner)**
* 基于MLLM、世界模型已有产品级交付案例
* 基于VLA、扩散模型的技术仍在探索阶段
#### 基于MLLM的一段式端到端
基于MLLM的端到端，也叫做基于**感知**的一段式端到端模型。  
* **PV感知**（透视图）：透视图模型则提供了一个更接近人类驾驶员视角的视图，它模拟了车辆前方的视角，有助于车辆识别和理解道路上的交通标志、行人和其他车辆。
* **BEV感知**（鸟瞰图）：鸟瞰视图模型提供了一个从上方观察车辆及其周围环境的视角。这种视角有助于车辆更好地理解交通流和车辆的位置关系，常用于路径规划和避障。
* **BEV Former** VS **Transformer** ：**BEVFormer**将*BEV感知*和*Transformer*技术融合，通过提取环视相机采集到的图像特征，并将提取的环视特征通过模型学习的方式转换到 BEV 空间（模型去学习如何将特征从 图像坐标系转换到 BEV 坐标系），从而实现 3D 目标检测和地图分割任务，并取得了 SOTA 的效果。
* **一段式端到端 UniAD**  

~~MLLM + 自动驾驶 $\neq$ 问答式自动驾驶~~
#### 基于世界模型的一段式端到端
世界模型（World Model）  

Sim2Real  
动作：Agent -> Virtual Environment(世界模型)  
反馈：Virtual Environment(世界模型) -> Agent

世界模型的争议点
* 感知世界VS预测未来
* 模拟现实VS做出决策

世界模型的流派众多
* 只做仿真的世界模型
* 仿真和Planner一起做的世界模型

世界模型和planner模型怎么配合？
* 当前环境 -> Planner -> 决策 -> 世界模型 -> 导致什么/遇到什么 -> Planner

#### 基于VLA的一段式端到端
VLA借助视觉大语言模型的能力，让**自然语言控制**（开放的命令）引入**感知算法**（模型能力强大），执行**决策算法**（实现开放的决策空间，不被人定义）  
VLM + Action = VLA   
自动驾驶中的VLA：**DriveMoE**

VLA的优点
* 参数量打，涌现能力强，擅长“人-车-环境”的多方、多模态交互
* 与MLLM没有本质区别，重点在最后输出的是action，而非轨迹点
* 更类人，更一统，更加的端到端

#### 基于扩散模型的一段式端到端
主要工作在自动驾驶的轨迹生成方案。使用了Diffusion Model，将一段初始化为噪声的凌乱轨迹，通过逐层去噪，得到清晰、符合要求的轨迹。

## 端到端开源数据集
### NuScenes
#### NuPlan：Planner
* 1200小时的真实驾驶数据
* 有人工标注的地图信息（道路、车道线等）
* 用AI对目标框进行了自动标注
* 提供开环、闭环仿真工具
#### NuImages：2D感知
* 93k视频，折合150小时的驾驶
* 93k带标注的图片和1100k无标注的图片
#### NuScenes：3D多模态感知
* 几乎涵盖了所有感知任务
### Bench2Drive
* 13638个视频，2000k完整标注的图片
* 测试集包含了220段路线，每段长度约150米，场景各不相同，涵盖44中驾驶技巧
* 提供了开环、闭环测试工具
* 可以进行传感器仿真
### 其他数据集
* Kitti、BDD
* Open Occupancy
* OpenAD

## 自动驾驶仿真器
**Carla Simulator**：可用于闭环测试，模拟自动驾驶的几乎每个环节（感知、定位、规控），可自行选择环境，可以选择周围出现车辆、行人、交通标志、红绿灯等，车辆还可选择行驶风格、车速等，可选择自车安装的传感器，系统会为每种传感器都模拟采集到的数据，可导出作为数据集。

## 端到端算法的评价指标
### 开环评测
只被动播放数据，环境不响应模型规划的动作
* L2 distance：模型规划轨迹与GT轨迹
* 碰撞率：与环境障碍物碰撞的比例
### 闭环评测
需要搭建仿真系统，环境相应模型动作
* Route Complete（RC）：路线完成率
* Infraction Score（IS）：违规率
* Drive Score（DS）：上述二者乘积
### 实车评测
最真实，最重要
* 接管率
* 舒适率：速度平滑性、转弯平滑性
* 安全性：与环境障碍物的最小距离
* 效率

## 大语言模型LLM
**三大核心技术**
1. Transformer类架构（⭐）
2. 海量数据预训练
3. 微调精调技术（⭐）

### Transformer
### ViT：Viston Transformer
### CLIP：Contrastive Language-Image Pre-training
zero-shot
### LLaVA：Visual Instruction Tuning

## BEV感知
BEV：Bird-Eye-View，鸟瞰视角/俯视视角。当前主流自动驾驶感知、规控技术都应用在该视角下
PV：Perspective View，每个相机上单独做感知，检出结果都是对应相机上的图像坐标信息
PV->BEV：在图片上感知，得到目标坐标点，再经坐标变换，投射到BEV视角
* 2D检测结果，投射后是一篇扇形区域
* 3D检测结果，投射后仍是一个3D框
### 空间坐标系
规定使用**右手原则**
* 世界坐标系
* 车辆坐标系
* 传感器坐标系

  每个传感器在车上的安装位置是固定的，相对位置可以用一个旋转+一个平移来表达

  所有坐标变换都能写成 **\[下一个坐标\] = \[上一个坐标\] x \[变换矩阵\]** 来表达  

  变换矩阵设计的所有参数统称为标定参数
  * 内参矩阵：传感器内部的坐标变换参数，如焦距、像素高宽等
  * 外参矩阵：传感器与外部其他传感器、世界坐标之间的变换参数，如相机安装位置、角度
#### 相机坐标系（相机）  
* 像素坐标系 -> 图像坐标系 -> 相机坐标系

### BEVFormer
BEVFormer，乃至端到端算法的里程碑   
![BEVFormer](./img/End-to-End_Autonomous_Driving/BEVFormer.PNG)

基于BEV的Transformer，重心在于如何处理BEV特征，可接下游任何的感知任务

后续工作开发出了轨迹规划任务，即成为端到端划时代之作UniAD

#### BEVFormer内容
* 多camera输入和特征提取：使用backbone模型（VGG、ResNet、ViT）得到图像特征
* Spatial Cross-Attention(空间交叉注意力)：从每个相机的图像特征中提取**BEV特征**，做Cross-Attention
	* BEV视角下，将自车前后左右一定长宽范围内的2D空间预设分辨率，划分成2D栅格，栅格总数记为 *M* x *N*
	* 每个栅栅格(x, y)，预设一组固定的高度 z1, z2, ……
	* 每个三位位置点（x, y, z），通过坐标变换，映射到相机上，每个点可能映射到多个相机上	*(和3dgs好像，从3D点云投影映射到2D像素)*
	* 在所有映射到的相机位置，随机提取附近的图像特征
	* 对每个相机vi，将采集到的所有图像特征，进行加权平均，作为（x, y, zi）在相机vi采集到的特征，记为f(x, y, zi, vi)
	* 由于相机的图正图像维度是D，总共有 *M* x *N* 个(x, y)栅格，故经过这种采样后，提取到的特征维度为 *M* x *N* x *D*
	* 将提取到的BEV特征，变换token实现多模态对齐，进行Cross-Attention
* Temporal Self-Attention(时间自注意力)
	* 上一刻的BEV特征 $B_{t-1}$ ，先根据自车移动向量，对齐到当前时刻 $B_{t-1}^{'}$ 
	* 当前时刻的BEV Query，自己与自己做deformable self-attn
	* 特别的，deformable attn中的偏移量，改为由 Q 与$B_{t-1}^{'}$ 的concat后，再用一个小网络预测得到

使用 BEVFormer 可以进行目标检测，实例分割，栅格地图的任务

（之间的模型：LSS（Lift-Splat-Shoot）
（之后的工作：Lane Detection：MapTR，建立BEV特征，建立Query，Cross Attention建立特征，Head输出，监督训练）

BEVFormer：BEV感知+Transformer结合的代表作
BEV感知的基本架构：传感器 -> 基础backbone提取特征 -> 构建BEV特征 -> 构造query -> 构造head和loss

### Diffusion Model扩散模型
扩散模型本质是 **加噪过程和去噪过程** 。

`Denoising Diffusion Probabilistic Models`（NIPS 2020）是Diffusion Model开山制作，一句撼动了GAN在图像生成领域的霸主地位。
训练一个模型，输入为噪声图像和step数量，预测噪声的参数  

![Denoising Diffusion Probabilistic Models](./img/End-to-End_Autonomous_Driving/DenoisingDiffusionProbabilisticModels.PNG)

生成阶段
1. 给定一个纯噪声图和初始step数
2. 用模型根据输入图和step数，预测噪声的参数
3. 将噪声图减去用参数生成的噪声，得到去噪图
4. step数-1
5. 将去噪后的图像和更新后的step数再次输入模型，重复2~5步骤

假定噪声符合高斯分布，由此得到递推公式 $ x_{t} = \sqrt{a_{t}} * x_{t-1} + \sqrt{1-a_{t}} * \epsilon_{t} $ ，其中 $ x_{t} $ 为当前图像，$ x_{t-1} $ 为上一张图像，$ \epsilon_{t} $ 为对上一张图像加入的噪声。  
最终，通项公式为 $ x_{t} = \sqrt{\hat{a_{t}}} * x_{0} + \sqrt{1-\hat{a_{t}}} * \epsilon_{t} $ , 其中 $ \hat{a_{t}} = a_{1} * a_{2} * …… * a_{t}$

引入时间编码嵌入，让扩散模型知道自己在生成过程的哪个阶段，从而做出正确的决策。为模型提供了上下文信息，明白了当前处于去噪过程的哪个阶段。

扩散模型最终预测噪声，相比于预测去噪后的图像效果更好。这是因为在加噪过程中，图像逐渐被噪声淹没，图像几乎全是噪声，预测干净图像是一件非常困难的事情。预测噪声，由于噪声符合高斯分布，模型在每一个时间步的任务都是一致的，预测一个来自标准高斯分布的噪声，这降低了模型的学习难度。

### Reinforcement Learning强化学习
|  | 监督学习 | 强化学习 |
| ------- | ------- | ------- |
| 训练真值 | 标签 | 环境 | 
| 优化目标 | 最小化损失 | 最大化reward |
| 优化策略 | 梯度下降 | 梯度上升 | 

如果你能准确描述正确答案，推荐使用监督学习进行训练；  
如果你不能准确描述正确答案，但直到正确答案有哪些特点，错误答案有哪些特点，推荐使用强化学习进行训练

强化学习中，以Agent智能体为强化学习的训练目标，由`环境`、`动作`、`状态`、`奖励`组成

策略函数，根据状态 $s$ ，描述动作$a$的概率分布的函数  
$ \pi(a|s) = P(A=a|S=s) $

状态转移函数，根据状态 $ s_{t} $ ，当采取了动作 $ a_{t} $ 后，描述下一个状态 $ s_{t+1} $ 的概率分布的函数  
$ p(s_{t+1}|s_{t}, a_{t}) = P(S_{t+1}=s_{t+1}|S=s_{t}, A=a_{t}) $

动作价值函数，已知策略函数$ \pi $、状态$ s_{t} $、采取的动作$ a_{t} $，描述该动作所获的**奖励**期望  
$ Q_{\pi}(s_{t}, a_{t}) = r(s_{t}, a_{t}) + \gamma \sum_{s_{t+1} \in S} p(s_{t+1}|s_{t}, a_{t}) \sum_{a_{t+1} \in A} \pi(a_{t+1}|s_{t+1})Q_{\pi}(s_{t+1}, a_{t+1}) $

## 两段式端到端
算法架构：**Data** -> **Perception** -> **Planner**   
* 传统算法框架到端到端算法的过渡阶段
* Planner模型化，解决线性planner重度依赖规则的问题（让Planner变得更强，能逐渐把Perception融入Planner）
* 仍具有现行架构的感知不准、信息损耗等问题
* 适合新手入门

### PLUTO
AI模型在nuPlan首次击败基于规则的planner PDM（PDM：2023 nuPlan planning challenge冠军）

此前AI方案的弱项
* 不擅长做横向行为建模（横向行为是方向盘的方向控制，纵向行为是车辆加速度）
* 开环评测与闭环评测结论差异大
* 总学到short-cut，混稀因果（例如跟车情况下，对红绿灯判断弱，导致前方车辆卡绿灯通过，车辆依旧跟车通过了红灯）

Query-based网络结构
* 对横向、纵向规划单独建模
* 在构建query的过程中将横向、纵向融合，增强模型同时规划横纵向行为的能力

设计辅助loss
* 提出了一种差分插值辅助loss
* 引入多项辅助训练任务，且对batch-wise运算友好，便于大规模应用于当代AI模型

数据增强
* 扰动，dropout，insertion
* 用以缩小开环评测与闭环评测的差异

对比学习
* 与数据增强配合使用，引入对比损失
* 用以抑制模型学习short-cut

## 一段式端到端
算法架构：**Data** -> **End2End(Planner)**

实际：仍显式地带有感知模块，只是感知和Planner模块是一同训练/端到端训练，因此都视为端到端
* 感知和Planner一同训练，有助于增强解释性，方便迭代调优

Planner的输出有两种形式
* 轨迹
* 驾驶动作

### 基于感知的端到端模型
对感知模型做改动、缝合