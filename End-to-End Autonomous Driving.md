# 端到端自动驾驶
上一代算法架构：**Data** -> **Perception** -> **Planner**  

上一代算法架构的缺陷：
* 感知不准
* 规控不拟人，覆盖不了长尾问题
* 感知-规控的衔接有信息损耗

端到端算法架构：**Data** -> **End2End(Planner)** （既干感知算法又干规控算法）   

多模态大语言模型的兴起，让人们看到信息大融合带来的无限创造力

四大分支方向：
1. MLLM（多模态大语言模型）
2. 世界模型
3. VLA（Vision Language Action Model）
4. 扩散模型

## 端到端算法分类
### 两段式端到端
算法架构：**Data** -> **Perception** -> **Planner**  
* 传统算法框架到端到端算法的过渡阶段
* Planner模型化，解决线性planner重度依赖规则的问题（让Planner变得更强，能逐渐把Perception融入Planner）
* 仍具有现行架构的感知不准、信息损耗等问题
* 适合新手入门
### 一段式端到端
算法架构：**Data** -> **End2End(Planner)**
* 基于MLLM、世界模型已有产品级交付案例
* 基于VLA、扩散模型的技术仍在探索阶段
#### 基于MLLM的一段式端到端
基于MLLM的端到端，也叫做基于**感知**的一段式端到端模型。  
* **PV感知**（透视图）：透视图模型则提供了一个更接近人类驾驶员视角的视图，它模拟了车辆前方的视角，有助于车辆识别和理解道路上的交通标志、行人和其他车辆。
* **BEV感知**（鸟瞰图）：鸟瞰视图模型提供了一个从上方观察车辆及其周围环境的视角。这种视角有助于车辆更好地理解交通流和车辆的位置关系，常用于路径规划和避障。
* **BEV Former** VS **Transformer** ：**BEVFormer**将*BEV感知*和*Transformer*技术融合，通过提取环视相机采集到的图像特征，并将提取的环视特征通过模型学习的方式转换到 BEV 空间（模型去学习如何将特征从 图像坐标系转换到 BEV 坐标系），从而实现 3D 目标检测和地图分割任务，并取得了 SOTA 的效果。
* **一段式端到端 UniAD**  

~~MLLM + 自动驾驶 $\neq$ 问答式自动驾驶~~
#### 基于世界模型的一段式端到端
世界模型（World Model）  

Sim2Real  
动作：Agent -> Virtual Environment(世界模型)  
反馈：Virtual Environment(世界模型) -> Agent

世界模型的争议点
* 感知世界VS预测未来
* 模拟现实VS做出决策

世界模型的流派众多
* 只做仿真的世界模型
* 仿真和Planner一起做的世界模型

世界模型和planner模型怎么配合？
* 当前环境 -> Planner -> 决策 -> 世界模型 -> 导致什么/遇到什么 -> Planner

#### 基于VLA的一段式端到端
VLA借助视觉大语言模型的能力，让**自然语言控制**（开放的命令）引入**感知算法**（模型能力强大），执行**决策算法**（实现开放的决策空间，不被人定义）  
VLM + Action = VLA   
自动驾驶中的VLA：**DriveMoE**

VLA的优点
* 参数量打，涌现能力强，擅长“人-车-环境”的多方、多模态交互
* 与MLLM没有本质区别，重点在最后输出的是action，而非轨迹点
* 更类人，更一统，更加的端到端

#### 基于扩散模型的一段式端到端
主要工作在自动驾驶的轨迹生成方案。使用了Diffusion Model，将一段初始化为噪声的凌乱轨迹，通过逐层去噪，得到清晰、符合要求的轨迹。

## 端到端开源数据集
### NuScenes
#### NuPlan：Planner
* 1200小时的真实驾驶数据
* 有人工标注的地图信息（道路、车道线等）
* 用AI对目标框进行了自动标注
* 提供开环、闭环仿真工具
#### NuImages：2D感知
* 93k视频，折合150小时的驾驶
* 93k带标注的图片和1100k无标注的图片
#### NuScenes：3D多模态感知
* 几乎涵盖了所有感知任务
### Bench2Drive
* 13638个视频，2000k完整标注的图片
* 测试集包含了220段路线，每段长度约150米，场景各不相同，涵盖44中驾驶技巧
* 提供了开环、闭环测试工具
* 可以进行传感器仿真
### 其他数据集
* Kitti、BDD
* Open Occupancy
* OpenAD

## 自动驾驶仿真器
**Carla Simulator**：可用于闭环测试，模拟自动驾驶的几乎每个环节（感知、定位、规控），可自行选择环境，可以选择周围出现车辆、行人、交通标志、红绿灯等，车辆还可选择行驶风格、车速等，可选择自车安装的传感器，系统会为每种传感器都模拟采集到的数据，可导出作为数据集。

## 端到端算法的评价指标
### 开环评测
只被动播放数据，环境不响应模型规划的动作
* L2 distance：模型规划轨迹与GT轨迹
* 碰撞率：与环境障碍物碰撞的比例
### 闭环评测
需要搭建仿真系统，环境相应模型动作
* Route Complete（RC）：路线完成率
* Infraction Score（IS）：违规率
* Drive Score（DS）：上述二者乘积
### 实车评测
最真实，最重要
* 接管率
* 舒适率：速度平滑性、转弯平滑性
* 安全性：与环境障碍物的最小距离
* 效率

## 大语言模型LLM
**三大核心技术**
1. Transformer类架构（⭐）
2. 海量数据预训练
3. 微调精调技术（⭐）

### Transformer
### ViT：Viston Transformer
### CLIP：Contrastive Language-Image Pre-training
zero-shot
### LLaVA：Visual Instruction Tuning

## BEV感知
BEV：Bird-Eye-View，鸟瞰视角/俯视视角。当前主流自动驾驶感知、规控技术都应用在该视角下
PV：Perspective View，每个相机上单独做感知，检出结果都是对应相机上的图像坐标信息
PV->BEV：在图片上感知，得到目标坐标点，再经坐标变换，投射到BEV视角
* 2D检测结果，投射后是一篇扇形区域
* 3D检测结果，投射后仍是一个3D框
### 空间坐标系
规定使用**右手原则**
* 世界坐标系
* 车辆坐标系
* 传感器坐标系

  每个传感器在车上的安装位置是固定的，相对位置可以用一个旋转+一个平移来表达

  所有坐标变换都能写成 **\[下一个坐标\] = \[上一个坐标\] x \[变换矩阵\]** 来表达  

  变换矩阵设计的所有参数统称为标定参数
  * 内参矩阵：传感器内部的坐标变换参数，如焦距、像素高宽等
  * 外参矩阵：传感器与外部其他传感器、世界坐标之间的变换参数，如相机安装位置、角度
#### 相机坐标系（相机）  
* 像素坐标系 -> 图像坐标系 -> 相机坐标系

### BEVFormer
BEVFormer，乃至端到端算法的里程碑   
![BEVFormer](./img/BEVFormer.png)

基于BEV的Transformer，重心在于如何处理BEV特征，可接下游任何的感知任务

后续工作开发出了轨迹规划任务，即成为端到端划时代之作UniAD

#### BEVFormer内容
* 多camera输入和特征提取：使用backbone模型（VGG、ResNet、ViT）得到图像特征
* Spatial Cross-Attention(空间交叉注意力)：从每个相机的图像特征中提取**BEV特征**，做Cross-Attention
	* BEV视角下，将自车前后左右一定长宽范围内的2D空间预设分辨率，划分成2D栅格，栅格总数记为 *M* x *N*
	* 每个栅栅格(x, y)，预设一组固定的高度 z1, z2, ……
	* 每个三位位置点（x, y, z），通过坐标变换，映射到相机上，每个点可能映射到多个相机上	*(和3dgs好像，从3D点云投影映射到2D像素)*
	* 在所有映射到的相机位置，随机提取附近的图像特征
	* 对每个相机vi，将采集到的所有图像特征，进行加权平均，作为（x, y, zi）在相机vi采集到的特征，记为f(x, y, zi, vi)
	* 由于相机的图正图像维度是D，总共有 *M* x *N* 个(x, y)栅格，故经过这种采样后，提取到的特征维度为 *M* x *N* x *D*
	* 将提取到的BEV特征，变换token实现多模态对齐，进行Cross-Attention
* Temporal Self-Attention(时间自注意力)
	* 上一刻的BEV特征 $B_{t-1}$ ，先根据自车移动向量，对齐到当前时刻 $B_{t-1}^{'}$ 
	* 当前时刻的BEV Query，自己与自己做deformable self-attn
	* 特别的，deformable attn中的偏移量，改为由 Q 与$B_{t-1}^{'}$ 的concat后，再用一个小网络预测得到

使用 BEVFormer 可以进行目标检测，实例分割，栅格地图的任务

（之间的模型：LSS（Lift-Splat-Shoot）
（之后的工作：Lane Detection：MapTR，建立BEV特征，建立Query，Cross Attention建立特征，Head输出，监督训练）

BEVFormer：BEV感知+Transformer结合的代表作
BEV感知的基本架构：传感器 -> 基础backbone提取特征 -> 构建BEV特征 -> 构造query -> 构造head和loss

## 两段式端到端
算法架构：**Data** -> **Perception** -> **Planner**   
* 传统算法框架到端到端算法的过渡阶段
* Planner模型化，解决线性planner重度依赖规则的问题（让Planner变得更强，能逐渐把Perception融入Planner）
* 仍具有现行架构的感知不准、信息损耗等问题
* 适合新手入门

### PLUTO
AI模型在nuPlan首次击败基于规则的planner PDM（PDM：2023 nuPlan planning challenge冠军）

此前AI方案的弱项
* 不擅长做横向行为建模（横向行为是方向盘的方向控制，纵向行为是车辆加速度）
* 开环评测与闭环评测结论差异大
* 总学到short-cut，混稀因果（例如跟车情况下，对红绿灯判断弱，导致前方车辆卡绿灯通过，车辆依旧跟车通过了红灯）

Query-based网络结构
* 对横向、纵向规划单独建模
* 在构建query的过程中将横向、纵向融合，增强模型同时规划横纵向行为的能力

设计辅助loss
* 提出了一种差分插值辅助loss
* 引入多项辅助训练任务，且对batch-wise运算友好，便于大规模应用于当代AI模型

数据增强
* 扰动，dropout，insertion
* 用以缩小开环评测与闭环评测的差异

对比学习
* 与数据增强配合使用，引入对比损失
* 用以抑制模型学习short-cut

## 一段式端到端
算法架构：**Data** -> **End2End(Planner)**

实际：仍显式地带有感知模块，只是感知和Planner模块是一同训练/端到端训练，因此都视为端到端
* 感知和Planner一同训练，有助于增强解释性，方便迭代调优

Planner的输出有两种形式
* 轨迹
* 驾驶动作

### 基于感知的端到端模型
对感知模型做改动、缝合